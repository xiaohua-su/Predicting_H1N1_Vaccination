{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting H1N1 Vaccination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./images/cdc.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a need to increase vaccination rates in the US. Having high vaccination rates will allow for another layer of safeguard against a certain disease by lowering the spread of it with the hopes of achieving herd immunity. With this added layer of protection for vaccine, people will not get severely sick from the disease or possibly die from it. As such, the CDC wants to create a model that can predict whether someone has gotten a vaccine. I will present to you model that specifically predicts whether or not someone got a H1N1 vaccine during the 2009-2010 H1N1 pandemic as there is survey data available for this vaccine. The model performed with an accuracy of 84% on unseen data with an F1 score of about 0.53. Using this model as the basis, a new model can then be created in order to locate which region many of these unvaccinated people are and strategies to get these individuals vaccinated such as free vaccine clinics, can then be further discussed after examination of the landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CDC wants to create a model that can accurately predict who has been vaccinated and who hasn't, in order to input into another model to locate these spots of where there is low vaccination rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, VotingClassifier, StackingRegressor\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score, roc_auc_score, plot_roc_curve\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "\n",
    "from model import *\n",
    "from get_features import *\n",
    "import pickle\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model library was taken from Flatiron's Workflow with pipelines lecture. Adjustments and addition of a method was added into the model.py. The get_feature file was taken from Haupt J. github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/training_set_features.csv')\n",
    "df2 = pd.read_csv('./data/training_set_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test set_feature was provided as well but unfortunately the set_labels that contain the target is with-held for the DataDriven competition. As such for the purposes of model testing, a train-test split will be performed on the training data provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.education.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.employment_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.employment_industry.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.employment_occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense as to why there would be so many missing values for the columns `employment_industry` and `employment_occupation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.health_insurance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.race.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to introduce racial bias into our model as such we'll be dropping this feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.child_under_6_months.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hhs_geo_region.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()\n",
    "#too many nulls in general to drop all of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning before Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([df,df2], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_status.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unemployed + not in labor force\n",
    "10231 + 1453 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_industry.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's is more null in than the total amount of people not actively working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how much nulls should remain if we replace those that had N/A to not applicable for the employment industry\n",
    "13330 - 11684 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It make sense that those that are not active in the workforce would not have not have an employment occupation and industry. As such, we will replace the occupation and industry to `not applicable` where there is a `not in Labor Force` and `unemployment` for `employment_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacement of some nan bases of whether they were employed or not as it makes sense that they would not have an industy\n",
    "#or occupation\n",
    "df3.loc[df3['employment_status'] == \"Not in Labor Force\", 'employment_industry'] = \"not_applicable\"\n",
    "df3.loc[df3['employment_status'] == \"Not in Labor Force\", 'employment_occupation'] = \"not_applicable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_industry.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_occupation.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[df3['employment_status'] == \"Unemployed\", 'employment_industry'] = \"not_applicable\"\n",
    "df3.loc[df3['employment_status'] == \"Unemployed\", 'employment_occupation'] = \"not_applicable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_industry.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.employment_occupation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is of binary or multilabel numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.h1n1_vaccine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.seasonal_vaccine.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df3.drop(['respondent_id','race', 'h1n1_vaccine', 'seasonal_vaccine'],\n",
    "            axis = 1)\n",
    "y = df3['h1n1_vaccine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Train-test-split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train,y_train], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df, y_vars= 'h1n1_vaccine') #graph makes sense looking at the dictionary for the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight Class imbalance where we can SMOTE if we want to. We will SMOTE to .35 to ensure that our data is closer to even split in terms of our target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = ['h1n1_concern', 'h1n1_knowledge', 'opinion_h1n1_vacc_effective',\n",
    "                  'opinion_h1n1_risk', 'opinion_h1n1_sick_from_vacc',\n",
    "                  'opinion_seas_vacc_effective', 'opinion_seas_risk',\n",
    "                  'opinion_seas_sick_from_vacc']\n",
    "\n",
    "cat_cols = ['behavioral_antiviral_meds', 'behavioral_avoidance',\n",
    "           'behavioral_face_mask','behavioral_wash_hands',\n",
    "           'behavioral_large_gatherings', 'behavioral_outside_home',\n",
    "           'behavioral_touch_face', 'doctor_recc_h1n1',\n",
    "           'doctor_recc_seasonal', 'chronic_med_condition',\n",
    "           'child_under_6_months', 'health_worker',\n",
    "           'health_insurance', 'sex', 'income_poverty',\n",
    "           'marital_status', 'rent_or_own', 'employment_status',\n",
    "           'hhs_geo_region', 'census_msa', 'household_adults',\n",
    "           'household_children', 'employment_industry', 'employment_occupation', 'age_group', 'education']\n",
    "\n",
    "cat_pipe = Pipeline(steps=[('cat_impute', SimpleImputer(strategy='most_frequent')),\n",
    "                              ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))])\n",
    "scale_pipe = Pipeline(steps=[('scale_impute', SimpleImputer(strategy='most_frequent')),\n",
    "                              ('scale', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[\n",
    "    ('cat', cat_pipe, cat_cols),\n",
    "    ('scale', scale_pipe, numeric)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for our simple model\n",
    "ct_no_cat = ColumnTransformer(transformers=[\n",
    "    ('scale', scale_pipe, numeric)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple models and It's corresponding dummy classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_simple = ImPipeline(steps=[\n",
    "    ('ct', ct_no_cat),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_results = ModelWithCV(dummy_simple, 'dummy', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all our numerical columns as our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple = X_train[numeric]\n",
    "y_simple = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple.columns == numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simple_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct_no_cat),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('logreg', LogisticRegression(random_state=1))\n",
    "]).fit(X_simple,y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_log = ModelWithCV(X_simple_pipe, 'logreg', X_simple, y_simple, cv_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_score = simple_log.cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = simple_log.plot_cv(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(X_simple_pipe, X_simple, y_simple);\n",
    "simple_log.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The f1, auc score and the accuracy is better than dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using kernel = linear as it needs to be this to get features\n",
    "svc2_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct_no_cat),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)), \n",
    "    ('svc2', SVC(random_state=1, kernel='linear'))]).fit(X_simple,y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv2_results = ModelWithCV(svc2_pipe, 'svc2', X_simple, y_simple, cv_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv2_score = sv2_results.cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax = sv2_results.plot_cv(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv2_results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has a better auc score but the accuracy and f1 is the same as dummy, so the default kernel for SVC is not good for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_p = ImPipeline(steps = [\n",
    "    ('ct', ct_no_cat),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('knn', KNeighborsClassifier())]).fit(X_simple,y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn = KNeighborsClassifier().fit(X_simple_trans, y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_result = ModelWithCV(knn_p, 'knn', X_simple, y_simple, cv_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_score = knn_result.cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_result.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lower accuracy but better f1 and auc score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple dtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct_no_cat),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('knn', DecisionTreeClassifier(random_state=1))]).fit(X_simple,y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtree_result = ModelWithCV(dtree_pipe, 'dt', X_simple, y_simple, cv_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_score = dtree_result.cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_result.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree has a lower accuracy but higher auc, and f1 score compared to the dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### let's put multiple default models in voting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingClassifier(estimators= [\n",
    "    ('lr', X_simple_pipe),\n",
    "    ('knn', knn_p),\n",
    "    ('dt', dtree_pipe)],\n",
    ").fit(X_simple,y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_result = ModelWithCV(voting, 'voting', X_simple, y_simple, cv_now = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_score = voting_result.cv_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(voting, X_simple, y_simple);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = voting.predict(X_simple)\n",
    "\n",
    "f1_score(y_simple, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use all our features this time around and scale/transofrm it out to see if it does any better in predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use all our features this time around and scale/transofrm it out to see if it does any better in predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipe = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('dummy', DummyClassifier(strategy='most_frequent'))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_results = ModelWithCV(dummy_pipe, 'dummy', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no grid search performed\n",
    "logreg_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('logreg', LogisticRegression(random_state=1))\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_result = ModelWithCV(logreg_pipe,'log_reg',X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_result.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When including all our features the acurracy increases from 78-79% to about 83%. IT is also better than our dummy in all aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### knn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_pickle = 'knn_pipe.sav'\n",
    "# pickle.dump(knn_pipe, open(knn_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_pipe = pickle.load(open('knn_pipe.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1_results = ModelWithCV(knn_pipe,'knn',X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "knn1_results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is higher than our logistic, the roc-auc is also higher but the accuracy is worse compared to logistic and our dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('dt', DecisionTreeClassifier(random_state=1))\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_pickle = 'dt_pipe.sav'\n",
    "# pickle.dump(dt_pipe, open(dt_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt_pipe = pickle.load(open('dt_pipe.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_results = ModelWithCV(dt_pipe,'dt',X_train,y_train) #0.7581627558662006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_results.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an accuracy score of 75% while the ROC, F1 and confusion matrix were of perfect scores of 1 and classification, indicates that this decision tree overfitted to our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('svm', SVC(random_state=1))\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_pickle = 'svm_pipe.sav'\n",
    "# pickle.dump(svm_pipe, open(svm_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_pipe = pickle.load(open('svm_pipe.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a long time to load\n",
    "cv_svm = ModelWithCV(svm_pipe,'svm',X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_svm.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is slightly worse than our knn model by about .04, while the accuracy is higher with the confusion matrix confirming it visually. The ROC-AUC is the same as the KNN with it being 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### voting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found the parameters for the logistic regression so we'll input this in our voting classifer\n",
    "logreg2_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('logreg', LogisticRegression(random_state=1,C=1, max_iter=50, penalty = 'l1', solver='saga' ))\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_complex = VotingClassifier(estimators= [\n",
    "     ('lr', logreg_pipe),\n",
    "     ('knn', knn_pipe),\n",
    "     ('dt', dt_pipe)\n",
    " ], n_jobs=-1).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_pickle = 'voting.sav'\n",
    "# pickle.dump(voting_complex, open(voting_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voting_complex = pickle.load(open('voting.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_complex_results = ModelWithCV(voting_complex, 'voting', X_train, y_train) # 0.834648027958063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting_complex_results.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(voting_complex, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = voting_complex.predict(X_train)\n",
    "\n",
    "f1_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_complex_results.cv_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### let's try out bagging instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bagging- no need to pickle as it doesn't take too long to run\n",
    "bagdt_pipe = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('bag', BaggingClassifier(random_state= 1))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagdt_result = ModelWithCV(bagdt_pipe, 'bagging', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagdt_result.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better than the decision tree by itself but it appears to also be super overfit with the f1 score being close to 1, and ROC-AUC being 1, while the accuracy is at 82%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rf', RandomForestClassifier(n_jobs=-1))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result = ModelWithCV(rf_pipe, 'bagging', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_result.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform hypertuning using grid search on both the bagging and random forest model to drop the overfitting of these models and then seeing how well it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### gridsearch bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagdt_pipe = ImPipeline(steps=[\n",
    "     ('ct', ct),\n",
    "     ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "     ('bag', BaggingClassifier(random_state= 1))\n",
    " ]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " bag_params = {\n",
    "     'bag__n_estimators' : [10,100,1000],\n",
    "     'bag__max_features' : [5,10,15,20],\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_grid = GridSearchCV(estimator=bagdt_pipe, param_grid=bag_params, n_jobs=-1).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_pickle = 'bag_gridsearch.sav'\n",
    "# pickle.dump(bag_grid, open(bag_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_grid = pickle.load(open('bag_gridsearch.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_tune = ModelWithCV(bag_grid.best_estimator_,'bag_tune', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bag_tune.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two features results came out to be on the upper boundary of what was set. While it is ideal to run another grid search, we just need to move on for now as we also have to grid search our random forest first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### grid search random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " rf_pipe = ImPipeline(steps=[\n",
    "     ('ct', ct),\n",
    "     ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "     ('rf', RandomForestClassifier(n_jobs=-1))\n",
    " ]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " rf_params = {\n",
    "     'rf__n_estimators' : [10 ,100,200],\n",
    "     'rf__criterion' : ['gini', 'entropy'],\n",
    "     'rf__max_depth' : [5,10,20,25],\n",
    "     'rf__min_samples_split' : [100,500,1000]\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " rf_grid = GridSearchCV(estimator=rf_pipe, param_grid=rf_params, n_jobs=-1,verbose=3).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pickle = 'rf_gridsearch.sav'\n",
    "pickle.dump(rf_grid, open(rf_pickle, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid = pickle.load(open('rf_gridsearch.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tune = ModelWithCV(rf_grid.best_estimator_,'bag_tune', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_tune.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is about the same as our SVC model at about 83% but the F1 and ROC-AUC is just slightly lower than the SVC. This does put this model in contention of for being the best model. The random forest model will be able to give us information on the feature importance of our model while the SVC gives a slightly better F1 of 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A SVC gridsearch was not perform due to the big O issue that would be encountered as the fitting time will increase quadratically for the amount of rows with this type of algorithim**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rfe with 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe2 = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rfe', RFE(RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100),\n",
    "               n_features_to_select= 60)),\n",
    "    ('rf', RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe2.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(rf_pipe2, 'rf_pipe2', X_train, y_train).print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(rf_pipe2.named_steps.rfe.support_.flatten(), index=get_feature_names(ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[features[0]== True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RFE of 100 selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe3 = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rfe', RFE(RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100),\n",
    "               n_features_to_select= 100)),\n",
    "    ('rf', RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100))\n",
    "], verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(rf_pipe3, 'rf_pipe3', X_train, y_train).print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE with 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe4 = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rfe', RFE(RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100),\n",
    "               n_features_to_select= 25)),\n",
    "    ('rf', RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100))\n",
    "], verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(rf_pipe4, 'rf_pipe3', X_train, y_train).print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RFE with all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe5 = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rfe', RFE(RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100),\n",
    "               n_features_to_select= 122)),\n",
    "    ('rf', RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100))\n",
    "], verbose=True).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(rf_pipe5, 'rf_pipe3', X_train, y_train).print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = pd.DataFrame(rf_pipe5.named_steps.rf.feature_importances_, index=get_feature_names(ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature1.sort_values(0, ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mapping out the OHE by manually looking at the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['behavioral_antiviral_meds', 'behavioral_avoidance',\n",
    "           'behavioral_face_mask','behavioral_wash_hands',\n",
    "           'behavioral_large_gatherings', 'behavioral_outside_home',\n",
    "           'behavioral_touch_face', 'doctor_recc_h1n1',\n",
    "           'doctor_recc_seasonal', 'chronic_med_condition',\n",
    "           'child_under_6_months', 'health_worker',\n",
    "           'health_insurance', 'sex', 'income_poverty',\n",
    "           'marital_status', 'rent_or_own', 'employment_status',\n",
    "           'hhs_geo_region', 'census_msa', 'household_adults',\n",
    "           'household_children', 'employment_industry', 'employment_occupation', 'age_group', 'education']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance according to our data:\n",
    "\n",
    "- behavioral_touch_face yes/no\n",
    "- opinion of the risk of h1n1\n",
    "- h1n1 vaccine effectiveness\n",
    "- season flu vaccine risk\n",
    "- season flu vaccine effectiveness\n",
    "- doc recommendation for the h1n1 vaccine\n",
    "- child under 6 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform two model evaluation as one is a slightly predictor but is a black box due to the kernel being used.The other can be used to gather feature importance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe2 = ImPipeline(steps=[\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('rfe', RFE(RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100),\n",
    "               n_features_to_select= 60)),\n",
    "    ('rf', RandomForestClassifier(criterion='gini', max_depth=25, min_samples_split=100, n_estimators = 100))\n",
    "]).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rf_pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(final_model, 'final_model', X_test, y_test).print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_pipe = ImPipeline(steps = [\n",
    "    ('ct', ct),\n",
    "    ('sm', SMOTE(sampling_strategy= 0.35, random_state=1)),\n",
    "    ('svm', SVC(random_state=1))\n",
    "]).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelWithCV(svm_pipe, 'final_model', X_test, y_test).print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both performed about the same with the SVC being slightly better as expected base on the difference in the training model. But overall, both performed only slightly worse on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further tuning of model is needed to lower the misclassification of those that did or did not get the H1N1 vaccine. Not only that but refinement can be made to the data if access to the original dataset as the regions in which each individual was located is masked. As such, we are unable to do initial analysis on where those who were unvaccinated are. We would also want to test the model against the covid-19 vaccine confidence survey, as they have some of the same questions that was asked in the H1N1 survey, to see how well it’s able to predict if someone had the covid-19 vaccine to examine if it’s generalizable for other vaccines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
